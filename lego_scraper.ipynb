{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 100.0.4896\n",
      "Get LATEST chromedriver version for 100.0.4896 google-chrome\n",
      "Driver [/Users/ESheldon/.wdm/drivers/chromedriver/mac64/100.0.4896.60/chromedriver] found in cache\n",
      "/var/folders/pk/qjr6ty3x7s58790y_7_dwl240000gn/T/ipykernel_4968/1553962433.py:38: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  self.driver = Chrome(ChromeDriverManager().install())\n"
     ]
    }
   ],
   "source": [
    "import selenium\n",
    "from selenium.webdriver import Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.common import service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "#from time import sleep, time\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "#import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import uuid\n",
    "import urllib\n",
    "\n",
    "\n",
    "'''\n",
    "This module contains the scraper class and its methods.\n",
    "'''\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self, url, search_term, headless=False):\n",
    "        options = Options()\n",
    "        if headless:\n",
    "            options.add_argument('--headless')\n",
    "            self.driver = Chrome(ChromeDriverManager().install(), options=options)\n",
    "        else:\n",
    "            self.driver = Chrome(ChromeDriverManager().install())\n",
    "        self.url = url\n",
    "        self.search_term = search_term.upper()\n",
    "        self.driver.get(self.url)\n",
    "   \n",
    "    def open_url(self, url):\n",
    "        self.driver.get(url)\n",
    "    \n",
    "    def search(self, XPATH):\n",
    "        search_bar = self.driver.find_element(By.XPATH, XPATH)\n",
    "        search_bar.click()\n",
    "        search_bar.send_keys(self.search_term)\n",
    "        search_bar.send_keys(u'\\ue007')\n",
    "\n",
    "    def click_button(self, XPATH):\n",
    "        button = self.driver.find_element(By.XPATH, XPATH)\n",
    "        button.click()\n",
    "\n",
    "    def scroll_up_top(self):\n",
    "        self.driver.execute_script(\"window.scrollTo(0,document.body.scrollTop)\")\n",
    "\n",
    "    def scroll_down_bottom(self):\n",
    "        self.driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "\n",
    "    def accept_cookies(self, frame_id, XPATH):\n",
    "        #time.sleep(2)\n",
    "        try:\n",
    "            if frame_id!=None:\n",
    "                self.switch_frame(frame_id)\n",
    "            else: pass\n",
    "            self.wait_for(XPATH)\n",
    "            self.click_button(XPATH)\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "    def wait_for(self, XPATH, delay = 10):\n",
    "        try:    \n",
    "            WebDriverWait(self.driver, delay).until(EC.presence_of_element_located((By.XPATH, XPATH)))\n",
    "        except TimeoutException:\n",
    "            print('Loading took too long. Timeout occurred.')\n",
    "\n",
    "    def switch_frame(self, frame_id):\n",
    "        self.wait_for(frame_id)\n",
    "        self.driver.switchTo().frame(frame_id)\n",
    "\n",
    "    def quit(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def next_page(self, url):\n",
    "        self.open_url(url)\n",
    "\n",
    "    def see_more(self, XPATH):\n",
    "        self.scroll_down_bottom()\n",
    "        self.click_button(XPATH)\n",
    "    \n",
    "    def infinite_scroll(self):\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            self.scroll_down_bottom()\n",
    "            time.sleep(3)   \n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "    def get_list_links(self, XPATH_container, XPATH_search_results, delay=10):\n",
    "        try: \n",
    "            self.scroll_down_bottom()\n",
    "            try:\n",
    "                self.see_more('//*[@id=\"search-more\"]/a')\n",
    "                self.infinite_scroll()\n",
    "                pass\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "            container = self.driver.find_element(By.XPATH, XPATH_container)\n",
    "            search_list = container.find_elements(By.XPATH, XPATH_search_results)\n",
    "\n",
    "            self.link_list = []\n",
    "\n",
    "            for result in search_list:\n",
    "                a_tag = result.find_element(By.TAG_NAME, 'a')\n",
    "                link = a_tag.get_attribute('href')\n",
    "                self.link_list.append(link)\n",
    "            \n",
    "            # print(self.link_list)\n",
    "            # print(len(self.link_list))\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            print('No results found. Try another search term.')\n",
    "            pass #don't want it to pass. Want it to start again.\n",
    "    \n",
    "    def get_img_links(self, XPATH_main_image, XPATH_thumbnail_container, XPATH_thumbnails):\n",
    "        self.img_list = []\n",
    "        try:\n",
    "            for link in self.link_list:\n",
    "                self.open_url(link)\n",
    "                individual_img_list = []\n",
    "                main_image = self.driver.find_element(By.XPATH, XPATH_main_image)\n",
    "                img_tag = main_image.find_element(By.TAG_NAME, 'img')\n",
    "                img_link = img_tag.get_attribute('src')\n",
    "                individual_img_list.append(img_link)\n",
    "                thumbnail_container = self.driver.find_element(By.XPATH, XPATH_thumbnail_container)\n",
    "                thumbnail_list = thumbnail_container.find_elements(By.XPATH, XPATH_thumbnails)\n",
    "                for thumbnail in thumbnail_list:\n",
    "                    img_tag = thumbnail.find_element(By.TAG_NAME, 'img')\n",
    "                    thumbnail_link = img_tag.get_attribute('src')\n",
    "                    individual_img_list.append(thumbnail_link)\n",
    "                self.img_list.append(individual_img_list)  \n",
    "        except NoSuchElementException:\n",
    "            self.individual_img_list.append('N/A')\n",
    "            self.img_list.append(self.individual_img_list)\n",
    "            pass\n",
    "        #print(self.img_list)\n",
    "   \n",
    "    # def get_info(self, XPATH_name, XPATH_date, XPATH_creator, XPATH_supporters, XPATH_days):\n",
    "    #     self.name_list = []\n",
    "    #     self.date_list = []\n",
    "    #     self.creator_list =[]\n",
    "    #     self.num_supporters_list = []\n",
    "    #     self.num_days_remaining_list = []\n",
    "    #     for link in self.link_list:\n",
    "    #         self.open_url(link)\n",
    "    #         self.get_details(lst_name = self.name_list, XPATH= XPATH_name)\n",
    "    #         self.get_details(lst_name = self.date_list, XPATH= XPATH_date)\n",
    "    #         self.get_details(lst_name = self.creator_list, XPATH= XPATH_creator)\n",
    "    #         self.get_details(lst_name = self.num_supporters_list, XPATH= XPATH_supporters)\n",
    "    #         self.get_details(lst_name = self.num_days_remaining_list, XPATH= XPATH_days)\n",
    "    #         # name = self.driver.find_element(By.XPATH, XPATH_name)\n",
    "    #         # self.name_list.append(name)\n",
    "    #         # date = self.driver.find_element(By.XPATH, XPATH_date)\n",
    "    #         # self.name_list.append(date)\n",
    "    \n",
    "    # def get_details(self, XPATH, lst_name):\n",
    "    #     detail = self.driver.find_element(By.XPATH, XPATH)\n",
    "    #     lst_name.append(detail)\n",
    "    \n",
    "    def create_uuid(self, result_list):\n",
    "        self.uuid_list = []\n",
    "        for i in range(len(result_list)):\n",
    "            UUID = str(uuid.uuid4())\n",
    "            self.uuid_list.append(UUID)\n",
    "    \n",
    "    # def collate_info(self):\n",
    "    #     self.info = {\"id\": self.link_id,\n",
    "    #             \"uuid\": self.link_uuid,\n",
    "    #             \"URL\": self.link_list,\n",
    "    #             \"idea_name\": self.name_list,\n",
    "    #             \"date\": self.date_list,\n",
    "    #             \"creator\": self.creator_list,\n",
    "    #             \"number_of_supporters\": self.num_supporters_list,\n",
    "    #             \"number_of_days_remaining\": self.num_days_remaining_list,\n",
    "    #             \"image_links\": self.img_list}\n",
    "    #     print(self.info) #delete later\n",
    "    #     return self.info\n",
    "\n",
    "    def get_html(self, url):\n",
    "        r = requests.get(url)\n",
    "        self.soup = bs(r.text, 'html.parser')\n",
    "\n",
    "    def find_in_html(self, tag, attribute, attribute_name):\n",
    "        self.soup.find(tag, {attribute: attribute_name}).text\n",
    "\n",
    "    # def get_info_from_html(self):\n",
    "    #     self.name_list = []\n",
    "    #     self.date_list = []\n",
    "    #     self.creator_list =[]\n",
    "        \n",
    "    #     for link in self.link_list:\n",
    "    #         self.get_html(link)\n",
    "    #         name = self.soup.find('h1').text\n",
    "    #         self.name_list.append(name)\n",
    "\n",
    "    #         date = self.soup.find('span', {\"class\":\"published-date\"}).text\n",
    "    #         self.date_list.append(date)\n",
    "\n",
    "    #         creator_name = self.soup.find('a', {'data-axl':\"alias\"}).text\n",
    "    #         self.creator_list.append(creator_name)\n",
    "\n",
    "    # def get_info_from_java(self):\n",
    "    #     self.num_supporters_list = []\n",
    "    #     self.num_days_remaining_list = []\n",
    "    #     for link in self.link_list:\n",
    "    #         self.driver.get(link)\n",
    "    #         soup = bs(self.driver.page_source, 'html.parser')\n",
    "    #         numbers = soup.findAll('div', class_= \"count\")\n",
    "    #         self.num_supporters_list.append(numbers[0].text)\n",
    "    #         self.num_days_remaining_list.append(numbers[1].text)\n",
    "\n",
    "    def download_raw_data(self,path='.', file_name='raw_data'):\n",
    "        if not os.path.exists(f'{path}/{file_name}'):\n",
    "            os.makedirs(f'{path}/{file_name}')\n",
    "        with open (f'{path}/{file_name}/data.json', 'w') as f:\n",
    "            json.dump(self.info, f, indent=\"\")\n",
    "\n",
    "\n",
    "    def download_images(self, path='.'):\n",
    "        if not os.path.exists(f'{path}/{self.search_term}'):\n",
    "            os.makedirs(f'{path}/{self.search_term}')\n",
    "\n",
    "        for i, lst in enumerate(self.img_list):\n",
    "            for j, img in enumerate(lst):\n",
    "                urllib.request.urlretrieve(img, f'{path}/{self.search_term}/{self.search_term}{i}.{j}.webp')\n",
    "\n",
    "class LegoScraper(Scraper):\n",
    "    def get_info_from_java(self):\n",
    "        self.num_supporters_list = []\n",
    "        self.num_days_remaining_list = []\n",
    "        for link in self.link_list:\n",
    "            self.driver.get(link)\n",
    "            soup = bs(self.driver.page_source, 'html.parser')\n",
    "            numbers = soup.findAll('div', class_= \"count\")\n",
    "            self.num_supporters_list.append(numbers[0].text)\n",
    "            self.num_days_remaining_list.append(numbers[1].text)\n",
    "        #print(self.num_supporters_list)\n",
    "    \n",
    "    def create_id(self):\n",
    "        self.id_list = []\n",
    "        for i in range(len(self.link_list)):\n",
    "            ID = self.link_list[i][-12:]\n",
    "            self.id_list.append(ID)\n",
    "\n",
    "    def explore_product_ideas(self, XPATH1, XPATH2):\n",
    "        self.click_button(XPATH1)\n",
    "        self.click_button(XPATH2)\n",
    "\n",
    "    def get_info_from_html(self):\n",
    "        self.name_list = []\n",
    "        self.date_list = []\n",
    "        self.creator_list =[]\n",
    "        \n",
    "        for link in self.link_list:\n",
    "            self.get_html(link)\n",
    "            name = self.soup.find('h1').text\n",
    "            self.name_list.append(name)\n",
    "\n",
    "            date = self.soup.find('span', {\"class\":\"published-date\"}).text\n",
    "            self.date_list.append(date)\n",
    "\n",
    "            creator_name = self.soup.find('a', {'data-axl':\"alias\"}).text\n",
    "            self.creator_list.append(creator_name)\n",
    "\n",
    "    def collate_info(self):\n",
    "        self.info = {\"id\": self.link_id,\n",
    "                \"uuid\": self.link_uuid,\n",
    "                \"URL\": self.link_list,\n",
    "                \"idea_name\": self.name_list,\n",
    "                \"date\": self.date_list,\n",
    "                \"creator\": self.creator_list,\n",
    "                \"number_of_supporters\": self.num_supporters_list,\n",
    "                \"number_of_days_remaining\": self.num_days_remaining_list,\n",
    "                \"image_links\": self.img_list}\n",
    "        #return self.info\n",
    "\n",
    "class CollocationsScraper(Scraper):\n",
    "    def create_dict(self):\n",
    "        self.info = {\n",
    "        # \"id\": self.link_id,\n",
    "        #         \"uuid\": self.link_uuid,\n",
    "                \"adj_rank-word-frequency\": [],\n",
    "                \"adj_phrases\": [],\n",
    "                \"verb_rank-word-frequency\": [],\n",
    "                \"verb_phrases\": []}\n",
    "        #print(self.info)\n",
    "\n",
    "    # def collate_data(self):\n",
    "    #     self.info = {\n",
    "    #     # \"id\": self.link_id,\n",
    "    #     #         \"uuid\": self.link_uuid,\n",
    "    #             \"adj_rank-word-frequency\": self.adj_frequency_list,\n",
    "    #             #\"adj_phrases\": self.adj_phrase_list,\n",
    "    #             \"verb_rank-word-frequency\": self.verb_frequency_list,\n",
    "    #             \"verb_phrases\": self.verb_phrase_list}\n",
    "    #     print(self.info)\n",
    "    def get_words(self):        \n",
    "        self.list_words = []\n",
    "    # container_results = driver.find_element(By.XPATH, '//ul[@class=\"sentence-mode result-group list-group mt-4 \"]')\n",
    "# list_results = container_results.find_elements(By.XPATH, './li')\n",
    "        words = self.driver.find_elements(By.XPATH, '//span[@class=\"  result font-weight-bold text-success \"]')\n",
    "# for result in list_results:\n",
    "#     words = result.find_elements(By.XPATH, '//span[@class=\"  result font-weight-bold text-success \"]')\n",
    "        for word in words:\n",
    "            self.list_words.append(word.text)\n",
    "        print(self.list_words)\n",
    "\n",
    "    def get_infinitives(self):\n",
    "        list_infinitives = []\n",
    "        for word in self.list_words:\n",
    "            dictionary_url = f'https://es.thefreedictionary.com/{word}'\n",
    "            self.driver.get(dictionary_url)\n",
    "            try:\n",
    "                accept_cookies = self.driver.find_element(By.XPATH, '/html/body/div[5]/div/div[1]/div/div/div[2]/a[1]')\n",
    "                accept_cookies.click()\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "            try:\n",
    "                infinitive = self.driver.find_element(By.tag_name, 'h1')\n",
    "                list_infinitives.append(infinitive.text)\n",
    "            except NoSuchElementException:\n",
    "                list_infinitives.append('N/A')\n",
    "            self.driver.close()\n",
    "        print(list_infinitives)\n",
    "    \n",
    "    def get_phrases(self, word_class):\n",
    "        self.adj_phrase_list = []\n",
    "        self.verb_phrase_list = []\n",
    "        new_url= f'{self.url}{word_class}/{self.search_term}'\n",
    "        self.get_html(new_url)\n",
    "        sentences = self.soup.findAll('li', {\"class\": \"btn-result list-group-item list-group-item-action\"})\n",
    "        if word_class == 'adj':\n",
    "            for sentence in sentences:\n",
    "                self.adj_phrase_list.append(sentence.text)\n",
    "                self.info[\"adj_phrases\"] = self.adj_phrase_list      \n",
    "        if word_class == 'v':\n",
    "            for sentence in sentences:\n",
    "                self.verb_phrase_list.append(sentence.text)\n",
    "                self.info[\"verb_phrases\"] = self.verb_phrase_list\n",
    "            # for sentence in sentences:\n",
    "        #     print(sentence.text)\n",
    "\n",
    "    def get_frequency(self, word_class):\n",
    "        self.adj_frequency_list = []\n",
    "        self.verb_frequency_list = []\n",
    "        new_url = f'{self.url}{word_class}/{self.search_term}?mode=frequency'\n",
    "        self.driver.get(new_url)\n",
    "        self.get_html(new_url)\n",
    "        words_frequency = self.soup.findAll('li', {\"class\": \"btn-result list-group-item list-group-item-action\"})\n",
    "        if word_class == 'adj':\n",
    "            for frequency in words_frequency:\n",
    "                str = frequency.text\n",
    "                stripped_str = str.strip()\n",
    "                self.adj_frequency_list.append(stripped_str)\n",
    "                self.info[\"adj_rank-word-frequency\"] = self.adj_frequency_list      \n",
    "        if word_class == 'v':\n",
    "            for frequency in words_frequency:\n",
    "                self.verb_frequency_list.append(frequency.text)\n",
    "                self.info[\"verb_rank-word-frequency\"] = self.verb_frequency_list  \n",
    "        # for frequency in words_frequency:\n",
    "        #     print(frequency.text)\n",
    "\n",
    "if __name__ == '__main__': \n",
    "\n",
    "    def web_scraper_lego():\n",
    "        #search_term = input('I would like to search for... ')\n",
    "        search_term = 'violin'\n",
    "        scraper = LegoScraper('https://ideas.lego.com', search_term)\n",
    "        try:\n",
    "            scraper.accept_cookies(frame_id=None, XPATH= '//button[@aria-label=\"Reject cookies\"]')\n",
    "            #scraper.explore_product_ideas('//a[@class=\"sub-menu\"][1]', '//div[@class=\"header-link\"][1]')\n",
    "            scraper.search('//input[@name=\"query\"]')\n",
    "            scraper.get_list_links('//*[@id=\"search_results\"]', './div')\n",
    "            time.sleep(2)\n",
    "            # scraper.get_img_links(XPATH_main_image='//div[@class=\"image-sizing-wrapper\"]', XPATH_thumbnail_container='//div[@class=\"thumbnails-tray\"]', XPATH_thumbnails='./div')\n",
    "            scraper.create_uuid(scraper.link_list)\n",
    "            # scraper.create_id()\n",
    "            #scraper.get_html()\n",
    "            # scraper.get_info_from_html()\n",
    "            scraper.get_info_from_java()\n",
    "            # #scraper.get_info()\n",
    "            # scraper.collate_info()\n",
    "            # scraper.download_raw_data()\n",
    "            #scraper.download_images()\n",
    "            #scraper.create_uuid()\n",
    "            # scraper.scroll_down_bottom()\n",
    "            # time.sleep(2)\n",
    "            # scraper.see_more('//*[@id=\"search-more\"]/a')\n",
    "            # #scraper.scroll_up_top()\n",
    "            # time.sleep(4)\n",
    "        finally: scraper.quit()\n",
    "\n",
    "    def web_scraper_collocations():\n",
    "        search_term = 'libertad'\n",
    "        scraper = CollocationsScraper('https://inspirassion.com/es/', search_term)\n",
    "        try:\n",
    "            scraper.search('//input[@id=\"query\"]')\n",
    "            scraper.create_dict()\n",
    "            scraper.get_frequency(word_class='adj')\n",
    "            scraper.get_phrases(word_class='adj')\n",
    "            scraper.get_frequency(word_class='v')\n",
    "            scraper.get_phrases(word_class='v')\n",
    "            #scraper.collate_data()\n",
    "            scraper.download_raw_data(file_name='raw_data_coll')\n",
    "            time.sleep(2)\n",
    "            # scraper.click_button('//span[@class=\"gl-new-dropdown-button-text\"]')\n",
    "            # scraper.click_button('//*[@class=\"gl-new-dropdown-item\"][6]')\n",
    "        finally: scraper.quit()\n",
    "    \n",
    "    #web_scraper_lego()\n",
    "    web_scraper_collocations()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81c5b488f73ff207468821e03890586d8ed279d3a86e8e8e46dd711fd28c910f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('data_coll_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
