{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 100.0.4896\n",
      "Get LATEST chromedriver version for 100.0.4896 google-chrome\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/100.0.4896.60/chromedriver_mac64.zip\n",
      "Driver has been saved in cache [/Users/ESheldon/.wdm/drivers/chromedriver/mac64/100.0.4896.60]\n",
      "/var/folders/pk/qjr6ty3x7s58790y_7_dwl240000gn/T/ipykernel_2172/1187078300.py:38: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  self.driver = Chrome(ChromeDriverManager().install())\n"
     ]
    }
   ],
   "source": [
    "import selenium\n",
    "from selenium.webdriver import Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.common import service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "#from time import sleep, time\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "#import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import uuid\n",
    "import urllib\n",
    "\n",
    "\n",
    "'''\n",
    "This module contains the scraper class and its methods.\n",
    "'''\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self, url, search_term, headless=False):\n",
    "        options = Options()\n",
    "        if headless:\n",
    "            options.add_argument('--headless')\n",
    "            self.driver = Chrome(ChromeDriverManager().install(), options=options)\n",
    "        else:\n",
    "            self.driver = Chrome(ChromeDriverManager().install())\n",
    "        self.url = url\n",
    "        self.search_term = search_term.upper()\n",
    "        self.driver.get(self.url)\n",
    "   \n",
    "    def open_url(self, url):\n",
    "        self.driver.get(url)\n",
    "    \n",
    "    def search(self, name=str):\n",
    "        search_bar = self.driver.find_element(By.NAME, name)\n",
    "        search_bar.click()\n",
    "        search_bar.send_keys(self.search_term)\n",
    "        search_bar.send_keys(u'\\ue007')\n",
    "\n",
    "    def click_button(self, XPATH):\n",
    "        button = self.driver.find_element(By.XPATH, XPATH)\n",
    "        button.click()\n",
    "\n",
    "    def scroll_up_top(self):\n",
    "        self.driver.execute_script(\"window.scrollTo(0,document.body.scrollTop)\")\n",
    "\n",
    "    def scroll_down_bottom(self):\n",
    "        self.driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "\n",
    "    def accept_cookies(self, frame_id, XPATH):\n",
    "        #time.sleep(2)\n",
    "        try:\n",
    "            if frame_id!=None:\n",
    "                self.switch_frame(frame_id)\n",
    "            else: pass\n",
    "            self.wait_for(XPATH)\n",
    "            self.click_button(XPATH)\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "    def wait_for(self, XPATH, delay = 10):\n",
    "        try:    \n",
    "            WebDriverWait(self.driver, delay).until(EC.presence_of_element_located((By.XPATH, XPATH)))\n",
    "        except TimeoutException:\n",
    "            print('Loading took too long. Timeout occurred.')\n",
    "\n",
    "    def switch_frame(self, frame_id):\n",
    "        self.wait_for(frame_id)\n",
    "        self.driver.switchTo().frame(frame_id)\n",
    "\n",
    "    def quit(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def next_page(self, url):\n",
    "        self.open_url(url)\n",
    "\n",
    "    def see_more(self, XPATH):\n",
    "        self.scroll_down_bottom()\n",
    "        self.click_button(XPATH)\n",
    "        \n",
    "    def explore_product_ideas(self, XPATH1, XPATH2):\n",
    "        self.click_button(XPATH1)\n",
    "        self.click_button(XPATH2)\n",
    "    \n",
    "    def infinite_scroll(self):\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            self.scroll_down_bottom()\n",
    "            time.sleep(3)   \n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "    def get_list_links(self, XPATH_container, XPATH_search_results, delay=10):\n",
    "        try: \n",
    "            self.scroll_down_bottom()\n",
    "            try:\n",
    "                self.see_more('//*[@id=\"search-more\"]/a')\n",
    "                self.infinite_scroll()\n",
    "                pass\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "            container = self.driver.find_element(By.XPATH, XPATH_container)\n",
    "            search_list = container.find_elements(By.XPATH, XPATH_search_results)\n",
    "\n",
    "            self.link_list = []\n",
    "\n",
    "            for result in search_list:\n",
    "                a_tag = result.find_element(By.TAG_NAME, 'a')\n",
    "                link = a_tag.get_attribute('href')\n",
    "                self.link_list.append(link)\n",
    "            \n",
    "            # print(self.link_list)\n",
    "            # print(len(self.link_list))\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            print('No results found. Try another search term.')\n",
    "            pass #don't want it to pass. Want it to start again.\n",
    "    \n",
    "    def get_img_links(self, XPATH_main_image, XPATH_thumbnail_container, XPATH_thumbnails):\n",
    "        self.img_list = []\n",
    "        try:\n",
    "            for link in self.link_list:\n",
    "                self.open_url(link)\n",
    "                individual_img_list = []\n",
    "                main_image = self.driver.find_element(By.XPATH, XPATH_main_image)\n",
    "                img_tag = main_image.find_element(By.TAG_NAME, 'img')\n",
    "                img_link = img_tag.get_attribute('src')\n",
    "                individual_img_list.append(img_link)\n",
    "                thumbnail_container = self.driver.find_element(By.XPATH, XPATH_thumbnail_container)\n",
    "                thumbnail_list = thumbnail_container.find_elements(By.XPATH, XPATH_thumbnails)\n",
    "                for thumbnail in thumbnail_list:\n",
    "                    img_tag = thumbnail.find_element(By.TAG_NAME, 'img')\n",
    "                    thumbnail_link = img_tag.get_attribute('src')\n",
    "                    individual_img_list.append(thumbnail_link)\n",
    "                self.img_list.append(individual_img_list)  \n",
    "        except NoSuchElementException:\n",
    "            self.individual_img_list.append('N/A')\n",
    "            self.img_list.append(self.individual_img_list)\n",
    "            pass\n",
    "        #print(self.img_list)\n",
    "   \n",
    "    # def get_info(self, XPATH_name, XPATH_date, XPATH_creator, XPATH_supporters, XPATH_days):\n",
    "    #     self.name_list = []\n",
    "    #     self.date_list = []\n",
    "    #     self.creator_list =[]\n",
    "    #     self.num_supporters_list = []\n",
    "    #     self.num_days_remaining_list = []\n",
    "    #     for link in self.link_list:\n",
    "    #         self.open_url(link)\n",
    "    #         self.get_details(lst_name = self.name_list, XPATH= XPATH_name)\n",
    "    #         self.get_details(lst_name = self.date_list, XPATH= XPATH_date)\n",
    "    #         self.get_details(lst_name = self.creator_list, XPATH= XPATH_creator)\n",
    "    #         self.get_details(lst_name = self.num_supporters_list, XPATH= XPATH_supporters)\n",
    "    #         self.get_details(lst_name = self.num_days_remaining_list, XPATH= XPATH_days)\n",
    "    #         # name = self.driver.find_element(By.XPATH, XPATH_name)\n",
    "    #         # self.name_list.append(name)\n",
    "    #         # date = self.driver.find_element(By.XPATH, XPATH_date)\n",
    "    #         # self.name_list.append(date)\n",
    "    \n",
    "    # def get_details(self, XPATH, lst_name):\n",
    "    #     detail = self.driver.find_element(By.XPATH, XPATH)\n",
    "    #     lst_name.append(detail)\n",
    "    \n",
    "    def create_id(self):\n",
    "        self.link_id = []\n",
    "        self.link_uuid = []\n",
    "        for i in range(len(self.link_list)):\n",
    "            ID = self.link_list[i][-12:]\n",
    "            UUID = str(uuid.uuid4())\n",
    "            self.link_id.append(ID)\n",
    "            self.link_uuid.append(UUID)\n",
    "    \n",
    "    def collate_info(self):\n",
    "        self.info = {\"id\": self.link_id,\n",
    "                \"uuid\": self.link_uuid,\n",
    "                \"URL\": self.link_list,\n",
    "                \"idea_name\": self.name_list,\n",
    "                \"date\": self.date_list,\n",
    "                \"creator\": self.creator_list,\n",
    "                \"number_of_supporters\": self.num_supporters_list,\n",
    "                \"number_of_days_remaining\": self.num_days_remaining_list,\n",
    "                \"image_links\": self.img_list}\n",
    "        print(self.info) #delete later\n",
    "        return self.info\n",
    "\n",
    "    def get_html(self, url):\n",
    "        r = requests.get(url)\n",
    "        self.soup = bs(r.text, 'html.parser')\n",
    "\n",
    "    def find_in_html(self, tag, attribute, attribute_name):\n",
    "        self.soup.find(tag, {attribute: attribute_name}).text\n",
    "\n",
    "    def get_info_from_html(self):\n",
    "        self.name_list = []\n",
    "        self.date_list = []\n",
    "        self.creator_list =[]\n",
    "        \n",
    "        for link in self.link_list:\n",
    "            self.get_html(link)\n",
    "            name = self.soup.find('h1').text\n",
    "            self.name_list.append(name)\n",
    "\n",
    "            date = self.soup.find('span', {\"class\":\"published-date\"}).text\n",
    "            self.date_list.append(date)\n",
    "\n",
    "            creator_name = self.soup.find('a', {'data-axl':\"alias\"}).text\n",
    "            self.creator_list.append(creator_name)\n",
    "\n",
    "    def get_info_from_java(self):\n",
    "        self.num_supporters_list = []\n",
    "        self.num_days_remaining_list = []\n",
    "        for link in self.link_list:\n",
    "            self.driver.get(link)\n",
    "            soup = bs(self.driver.page_source, 'html.parser')\n",
    "            numbers = soup.findAll('div', class_= \"count\")\n",
    "            self.num_supporters_list.append(numbers[0].text)\n",
    "            self.num_days_remaining_list.append(numbers[1].text)\n",
    "\n",
    "    def download_raw_data(self,path='.'):\n",
    "        if not os.path.exists(f'{path}/raw_data'):\n",
    "            os.makedirs(f'{path}/raw_data')\n",
    "        with open (f'{path}/raw_data/data.json', 'w') as f:\n",
    "            json.dump(self.info, f, indent=\"\")\n",
    "\n",
    "\n",
    "    def download_images(self, path='.'):\n",
    "        if not os.path.exists(f'{path}/{self.search_term}'):\n",
    "            os.makedirs(f'{path}/{self.search_term}')\n",
    "\n",
    "        for i, lst in enumerate(self.img_list):\n",
    "            for j, img in enumerate(lst):\n",
    "                urllib.request.urlretrieve(img, f'{path}/{self.search_term}/{self.search_term}{i}.{j}.webp')\n",
    "\n",
    "class LegoScraper(Scraper):\n",
    "    def get_info_from_java(self):\n",
    "        self.num_supporters_list = []\n",
    "        self.num_days_remaining_list = []\n",
    "        for link in self.link_list:\n",
    "            self.driver.get(link)\n",
    "            soup = bs(self.driver.page_source, 'html.parser')\n",
    "            numbers = soup.findAll('div', class_= \"count\")\n",
    "            self.num_supporters_list.append(numbers[0].text)\n",
    "            self.num_days_remaining_list.append(numbers[1].text)\n",
    "\n",
    "if __name__ == '__main__': \n",
    "\n",
    "    def web_scraper():\n",
    "        #search_term = input('I would like to search for... ')\n",
    "        search_term = 'violin'\n",
    "        scraper = LegoScraper('https://ideas.lego.com', search_term)\n",
    "        try:\n",
    "            scraper.accept_cookies(frame_id=None, XPATH= '//button[@aria-label=\"Reject cookies\"]')\n",
    "            #scraper.explore_product_ideas('//a[@class=\"sub-menu\"][1]', '//div[@class=\"header-link\"][1]')\n",
    "            scraper.search(name='query')\n",
    "            scraper.get_list_links('//*[@id=\"search_results\"]', './div')\n",
    "            time.sleep(2)\n",
    "            # scraper.get_img_links(XPATH_main_image='//div[@class=\"image-sizing-wrapper\"]', XPATH_thumbnail_container='//div[@class=\"thumbnails-tray\"]', XPATH_thumbnails='./div')\n",
    "            # scraper.create_id()\n",
    "            #scraper.get_html()\n",
    "            # scraper.get_info_from_html()\n",
    "            # scraper.get_info_from_java()\n",
    "            # #scraper.get_info()\n",
    "            # scraper.collate_info()\n",
    "            # scraper.download_raw_data()\n",
    "            #scraper.download_images()\n",
    "            #scraper.create_uuid()\n",
    "            # scraper.scroll_down_bottom()\n",
    "            # time.sleep(2)\n",
    "            # scraper.see_more('//*[@id=\"search-more\"]/a')\n",
    "            # #scraper.scroll_up_top()\n",
    "            # time.sleep(4)\n",
    "        finally: scraper.quit()\n",
    "\n",
    "\n",
    "    web_scraper()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81c5b488f73ff207468821e03890586d8ed279d3a86e8e8e46dd711fd28c910f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('data_coll_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
