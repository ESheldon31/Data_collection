{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def load_more(self, num_repeats=3):\n",
    "    #     for i in range(num_repeats):\n",
    "    #         self.scroll_down_bottom()\n",
    "    #         time.sleep(3)\n",
    "\n",
    "\n",
    "    # def click_button(self, class_name):\n",
    "    #     button = self.driver.find_element(By.CLASS_NAME, class_name)\n",
    "    #     button.click()\n",
    "\n",
    "    # def search(self, ID, search_term, XPATH):\n",
    "    #     self.search_bar(ID=str, search_term=str)\n",
    "    #     self.click_button(XPATH)\n",
    "\n",
    "    # def search(self, name=str, search_term=str, XPATH=str):\n",
    "    #     search_bar = self.driver.find_element(By.NAME, name)\n",
    "    #     search_bar.click()\n",
    "    #     search_bar.send_keys(search_term)\n",
    "    #     self.click_button(XPATH)\n",
    "\n",
    "    # def get_link(self, XPATH):\n",
    "    #     identifier = self.driver.find_element(By.XPATH, XPATH) \n",
    "    #     a_tag = identifier.find_element(By.TAG_NAME, 'a')   \n",
    "    #     link = a_tag.get_attribute('href')\n",
    "    #     self.open_url(link)\n",
    "\n",
    "\n",
    "    # def create_uuid(self):\n",
    "    #     link_uuid = []\n",
    "    #     for i in range(len(self.link_list)):\n",
    "    #         UUID = uuid.uuid4()\n",
    "    #         link_uuid.append(UUID)\n",
    "    #     print(link_uuid)\n",
    "\n",
    "    # def create_dict(self):\n",
    "    #     dict = {}\n",
    "    #     {'id': actual id, 'uuid': actual_uuid}\n",
    "\n",
    "                # info[\"id\"] = link_id.append(ID)\n",
    "            #     # \"uuid\": UUID, \n",
    "            #     # \"URL\": self.link_list[i]}\n",
    "\n",
    "def download_images(self, path='.'):\n",
    "    if not os.path.exists(f'{path}/{self.search_term}'):\n",
    "        os.makedirs(f'{path}/{self.search_term}')\n",
    "    \n",
    "    for i, img in enumerate(self.img_list):\n",
    "        urllib.request.urlretrieve(img, f'{path}/{self.search_term}/{self.search_term}{i}.png')\n",
    "\n",
    "lists = []\n",
    "# make list of lists\n",
    "for i in range(2): \n",
    "    # append list \n",
    "    lists.append([]) \n",
    "    for j in range(5): \n",
    "        lists[i].append(j) \n",
    "# Display result\n",
    "print(lists)\n",
    "\n",
    "import selenium\n",
    "from selenium.webdriver import Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "#import requests\n",
    "from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.common import service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "#from time import sleep, time\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "#import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import csv\n",
    "import uuid\n",
    "\n",
    "'''\n",
    "This module contains the scraper class and its methods.\n",
    "'''\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self, url, headless=False):\n",
    "        options = Options()\n",
    "        if headless:\n",
    "            options.add_argument('--headless')\n",
    "            self.driver = Chrome(ChromeDriverManager().install(), options=options)\n",
    "        else:\n",
    "            self.driver = Chrome(ChromeDriverManager().install())\n",
    "        self.url = url\n",
    "        self.driver.get(self.url)\n",
    "   \n",
    "    def open_url(self, url):\n",
    "        self.driver.get(url)\n",
    "    \n",
    "    def search(self, name=str, search_term=str):\n",
    "        search_bar = self.driver.find_element(By.NAME, name)\n",
    "        search_bar.click()\n",
    "        search_bar.send_keys(search_term)\n",
    "        search_bar.send_keys(u'\\ue007')\n",
    "\n",
    "    def click_button(self, XPATH):\n",
    "        button = self.driver.find_element(By.XPATH, XPATH)\n",
    "        button.click()\n",
    "\n",
    "    def scroll_up_top(self):\n",
    "        self.driver.execute_script(\"window.scrollTo(0,document.body.scrollTop)\")\n",
    "\n",
    "    def scroll_down_bottom(self):\n",
    "        self.driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "\n",
    "    def accept_cookies(self, frame_id, XPATH):\n",
    "        #time.sleep(2)\n",
    "        try:\n",
    "            if frame_id!=None:\n",
    "                self.switch_frame(frame_id)\n",
    "            else: pass\n",
    "            self.wait_for(XPATH)\n",
    "            self.click_button(XPATH)\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "    def wait_for(self, XPATH, delay = 10):\n",
    "        try:    \n",
    "            WebDriverWait(self.driver, delay).until(EC.presence_of_element_located((By.XPATH, XPATH)))\n",
    "        except TimeoutException:\n",
    "            print('Loading took too long. Timeout occurred.')\n",
    "\n",
    "    def switch_frame(self, frame_id):\n",
    "        self.wait_for(frame_id)\n",
    "        self.driver.switchTo().frame(frame_id)\n",
    "\n",
    "    def quit(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def next_page(self, url):\n",
    "        self.open_url(url)\n",
    "\n",
    "    def see_more(self, XPATH):\n",
    "        self.scroll_down_bottom()\n",
    "        self.click_button(XPATH)\n",
    "        \n",
    "    def explore_product_ideas(self, XPATH1, XPATH2):\n",
    "        self.click_button(XPATH1)\n",
    "        self.click_button(XPATH2)\n",
    "    \n",
    "    def infinite_scroll(self):\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            self.scroll_down_bottom()\n",
    "            time.sleep(3)   \n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "    def get_list_links(self, XPATH_container, XPATH_search_results, delay=10):\n",
    "        try: \n",
    "            self.scroll_down_bottom()\n",
    "            self.see_more('//*[@id=\"search-more\"]/a')\n",
    "            self.infinite_scroll()\n",
    "            container = self.driver.find_element(By.XPATH, XPATH_container)\n",
    "            search_list = container.find_elements(By.XPATH, XPATH_search_results)\n",
    "\n",
    "            self.link_list = []\n",
    "\n",
    "            for result in search_list:\n",
    "                a_tag = result.find_element(By.TAG_NAME, 'a')\n",
    "                link = a_tag.get_attribute('href')\n",
    "                self.link_list.append(link)\n",
    "            \n",
    "            print(self.link_list)\n",
    "            print(len(self.link_list))\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            print('No results found.')\n",
    "            pass\n",
    "\n",
    "    def create_id(self):\n",
    "        link_id = []\n",
    "        # link_uuid = []\n",
    "        info = {\"id\": link_id,\n",
    "                \"uuid\": [],\n",
    "                \"URL\": []}\n",
    "        for i in range(len(self.link_list)):\n",
    "            ID = self.link_list[i][-12:]\n",
    "            UUID = uuid.uuid4()\n",
    "            info[\"id\"] = link_id.append(ID), \n",
    "                \"uuid\": UUID, \n",
    "                \"URL\": self.link_list[i]}\n",
    "            # link_id.append(ID)\n",
    "            # link_uuid.append(UUID)\n",
    "        \n",
    "        #print(link_id)\n",
    "        # info = {\n",
    "        #     \"id\": ID, \n",
    "        #     \"uuid\": UUID\n",
    "        # }\n",
    "        print(info)\n",
    "        return info\n",
    "\n",
    "    # def create_uuid(self):\n",
    "    #     link_uuid = []\n",
    "    #     for i in range(len(self.link_list)):\n",
    "    #         UUID = uuid.uuid4()\n",
    "    #         link_uuid.append(UUID)\n",
    "    #     print(link_uuid)\n",
    "\n",
    "    # def create_dict(self):\n",
    "    #     dict = {}\n",
    "    #     {'id': actual id, 'uuid': actual_uuid}\n",
    "\n",
    "        \n",
    "        # for i, img in enumerate(self.img_list):\n",
    "        #     urllib.request.urlretrieve(img, f'{path}/{self.search_term}/{self.search_term}{i}.webp')\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "\n",
    "    def web_scraper():\n",
    "        scraper = Scraper('https://ideas.lego.com')\n",
    "        try:\n",
    "            scraper.accept_cookies(frame_id=None, XPATH= '//button[@aria-label=\"Reject cookies\"]')\n",
    "            #scraper.explore_product_ideas('//a[@class=\"sub-menu\"][1]', '//div[@class=\"header-link\"][1]')\n",
    "            scraper.search(name='query', search_term='piano')\n",
    "            scraper.get_list_links('//*[@id=\"search_results\"]', './div')\n",
    "            time.sleep(2)\n",
    "            scraper.create_id()\n",
    "            #scraper.create_uuid()\n",
    "            # scraper.scroll_down_bottom()\n",
    "            # time.sleep(2)\n",
    "            # scraper.see_more('//*[@id=\"search-more\"]/a')\n",
    "            # #scraper.scroll_up_top()\n",
    "            # time.sleep(4)\n",
    "        finally: scraper.quit()\n",
    "\n",
    "\n",
    "    web_scraper()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Name 1\n",
      "Enter Score 1\n",
      "Enter Name 2\n",
      "Enter Score 2\n",
      "[['Ellie', 100.0], ['Matt', 100.0]]\n"
     ]
    }
   ],
   "source": [
    "n =2\n",
    "lst = []\n",
    "for i in range(n):\n",
    "    print(\"Enter Name \" + str(i+1))\n",
    "    lst.append([input()])\n",
    "    for j in range(i,i+1):\n",
    "        print(\"Enter Score \" + str(j+1))\n",
    "        lst[i].append(float(input()))\n",
    "    \n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.0.0)\n",
      "b.0.1)\n",
      "c.0.2)\n",
      "d.1.0)\n",
      "e.1.1)\n",
      "f.1.2)\n",
      "g.2.0)\n",
      "h.2.1)\n",
      "i.2.2)\n"
     ]
    }
   ],
   "source": [
    "img_list = [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n",
    "\n",
    "for i, lst in enumerate(img_list):\n",
    "    for j, img in enumerate(lst):\n",
    "        print(f'{img}.{i}.{j}')\n",
    "# for i, lst in enumerate(img_list):\n",
    "#     print(f'{lst}.{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Trout object at 0x7fe203bf1280>\n",
      "The fish is swimming.\n"
     ]
    }
   ],
   "source": [
    "class Fish:\n",
    "    def __init__(self, first_name, last_name=\"Fish\", skeleton=\"bone\", eyelids=False):\n",
    "        self.first_name = first_name\n",
    "        self.last_name = last_name\n",
    "        self.skeleton = skeleton\n",
    "        self.eyelids = eyelids\n",
    "    \n",
    "    def swim(self):\n",
    "        print('The fish is swimming.')\n",
    "    \n",
    "    def swim_backwards(self):\n",
    "        print('The fish can swim backwards.')\n",
    "\n",
    "class Trout(Fish):\n",
    "    pass\n",
    "terry = Trout(\"Terry\")\n",
    "print(terry)\n",
    "terry.swim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 100.0.4896\n",
      "Get LATEST chromedriver version for 100.0.4896 google-chrome\n",
      "Driver [/Users/ESheldon/.wdm/drivers/chromedriver/mac64/100.0.4896.60/chromedriver] found in cache\n",
      "/var/folders/pk/qjr6ty3x7s58790y_7_dwl240000gn/T/ipykernel_3076/3631960302.py:18: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = Chrome(ChromeDriverManager().install())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['satisfacer', 'despertado', 'sintió', 'excitaron', 'tuve', 'picado', 'mostró', 'atraía', 'resistir', 'inspiraban', 'impulsadas', 'saciar', 'genera', 'movidos', 'estimularon', 'llama', 'atraída', 'vencer', 'llevado', 'satisfecha', 'causaba', 'vió', 'provoca', 'notó', 'contiene', 'demostrando', 'satisfecha', 'aguijoneado', 'convirtió', 'movió', 'revelaba', 'adivinó', 'deja', 'manifiesta', 'aumentar', 'visitar', 'interesar', 'alimentar', 'producen', 'crecer', 'dando', 'sienta', 'arrostrar', 'avivar', 'enseñar', 'refiere', 'desafiar', 'descubrir', 'experimentó', 'responder']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import selenium \n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os\n",
    "import json\n",
    "import selenium\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.support.select import Select\n",
    "import time\n",
    "\n",
    "driver = Chrome(ChromeDriverManager().install())\n",
    "url= 'https://inspirassion.com/es/v/curiosidad'\n",
    "driver.get(url)\n",
    "# soup = bs(driver.page_source, 'html.parser')\n",
    "# print(soup.prettify)\n",
    "\n",
    "# container_results = driver.find_element(By.XPATH, '//ul[@class=\"sentence-mode result-group list-group mt-4 \"]')\n",
    "# list_results = container_results.find_elements(By.XPATH, './li')\n",
    "# print(list_results)\n",
    "\n",
    "# list_words = []\n",
    "# for result in list_results:\n",
    "#     word = result.find_element(By.XPATH, '//span[@class=\"  result font-weight-bold text-success \"]')\n",
    "#     list_words.append(word.text)\n",
    "# print(list_words)\n",
    "\n",
    "# list_words = []\n",
    "# container_results = driver.find_element(By.XPATH, '//ul[@class=\"sentence-mode result-group list-group mt-4 \"]')\n",
    "# list_results = container_results.find_elements(By.XPATH, './li')\n",
    "# for i in range(2, len(list_results)):\n",
    "#     result = list_results[i]\n",
    "#     word = result.find_element(By.XPATH, '/span[@class=\"  result font-weight-bold text-success \"]')\n",
    "#     list_words.append(word.text)\n",
    "# print(list_words)\n",
    "\n",
    "# list_words = []\n",
    "# # container_results = driver.find_element(By.XPATH, '//ul[@class=\"sentence-mode result-group list-group mt-4 \"]')\n",
    "# # list_results = container_results.find_elements(By.XPATH, './li')\n",
    "# words = result.find_elements(By.XPATH, '//span[@class=\"  result font-weight-bold text-success \"]')\n",
    "# # for result in list_results:\n",
    "# #     words = result.find_elements(By.XPATH, '//span[@class=\"  result font-weight-bold text-success \"]')\n",
    "# for word in words:\n",
    "#     list_words.append(word.text)\n",
    "# print(list_words)\n",
    "\n",
    "list_words = []\n",
    "words = driver.find_elements(By.XPATH, '//span[@class=\"  result font-weight-bold text-success \"]')\n",
    "for word in words:\n",
    "    list_words.append(word.text)\n",
    "print(list_words)\n",
    "\n",
    "#list_infinitives = []\n",
    "# for word in list_words:\n",
    "#     dictionary_url = f'https://es.thefreedictionary.com/{word}'\n",
    "#     driver.get(dictionary_url)\n",
    "#     try:\n",
    "#         accept_cookies = driver.find_element(By.XPATH, '/html/body/div[5]/div/div[1]/div/div/div[2]/a[1]')\n",
    "#         accept_cookies.click()\n",
    "#     except NoSuchElementException:\n",
    "#         pass\n",
    "#     try:\n",
    "#         infinitive = driver.find_element(By.tag_name, 'h1')\n",
    "#         list_infinitives.append(infinitive.text)\n",
    "#     except NoSuchElementException:\n",
    "#         list_infinitives.append('N/A')\n",
    "#     driver.close()\n",
    "# print(list_infinitives)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def get_frequency(url):\n",
    "    new_url = url + '?mode=frequency'\n",
    "    driver.get(new_url)\n",
    "\n",
    "\n",
    "time.sleep(5)\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup = bs(driver.page_source, 'html.parser')\n",
    "\n",
    "# print(soup.prettify)\n",
    "\n",
    "# container_results = driver.find_element(By.XPATH, '//div[@class=\"result-group mt-3\"]')\n",
    "# list_results = driver.find_element(By.XPATH, '//a[class=\"btn-result btn btn-default btn-md gl-button mt-2 mr-1\"][1]')\n",
    "# #list_words = []\n",
    "# word = list_results.text\n",
    "# # for result in list_results:\n",
    "# #     word = result.text\n",
    "# #     list_words.append(word)\n",
    "# print(word)\n",
    "\n",
    "#button_dropdown.click()\n",
    "# # soup = bs(driver.page_source, 'html.parser')\n",
    "# r = requests.get(url)\n",
    "# soup = bs(r.text, 'html.parser')\n",
    "# sentences = soup.findAll('li', {\"class\": \"btn-result list-group-item list-group-item-action\"})\n",
    "# for sentence in sentences:\n",
    "#     print(sentence.text)\n",
    "# print(soup)\n",
    "#soup.find(tag, {attribute: attribute_name}).text\n",
    "#h1 = soup.find('h1').text\n",
    "# creator_name = soup.find('a', {'data-axl':\"alias\"}).text\n",
    "# support = soup.find('div', {\"class\":\"support-info\"})\n",
    "# stats = support.findAll('div', {\"class\": \"stats active\"})\n",
    "#numbers = soup.findAll('div', class_= \"count\")\n",
    "#span = soup.find('span', {\"class\":\"published-date\"}).text\n",
    "#print(numbers[0].text)\n",
    "# print(h1)\n",
    "# print(creator_name)\n",
    "# print(span)\n",
    "#print(numbers)\n",
    "\n",
    "# d = {\"name\": h1, \"creator\": creator_name, \"date\": span}\n",
    "# if not os.path.exists('./raw_data'):\n",
    "#     os.makedirs('./raw_data')\n",
    "# with open ('data.json', 'w') as f:\n",
    "#     json.dump(d, f, indent=\"\")\n",
    "\n",
    "#driver = Chrome(ChromeDriverManager().install())\n",
    "#         result.click()\n",
    "    \n",
    "#         pop_up_window = driver.switch_to.window(driver.window_handles[1])\n",
    "#         try:\n",
    "#             accept_cookies = driver.find_element(By.XPATH, '/html/body/div[5]/div/div[1]/div/div/div[2]/a[1]')\n",
    "#             accept_cookies.click()\n",
    "#         except NoSuchElementException:\n",
    "#             pass\n",
    "#         time.sleep(2)\n",
    "#         infinitive = driver.find_element(By.XPATH, '//h1')\n",
    "#         list_infinitives.append(infinitive.text)\n",
    "#         driver.close()\n",
    "#         driver.switch_to.window(driver.window_handles[0])\n",
    "#     except:\n",
    "#         list_infinitives.append('N/A')\n",
    "\n",
    "# print(list_infinitives)\n",
    "\n",
    "\n",
    "# try:\n",
    "#         result.click()\n",
    "#         pop_up_window = driver.switch_to.window(driver.window_handles[1])\n",
    "#         try:\n",
    "#             accept_cookies = driver.find_element(By.XPATH, '/html/body/div[5]/div/div[1]/div/div/div[2]/a[1]')\n",
    "#             accept_cookies.click()\n",
    "#         except NoSuchElementException:\n",
    "#             pass\n",
    "#         time.sleep(2)\n",
    "#         infinitive = driver.find_element(By.XPATH, '//h1')\n",
    "#         list_infinitives.append(infinitive.text)\n",
    "#         driver.close()\n",
    "#         driver.switch_to.window(driver.window_handles[0])\n",
    "#     except:\n",
    "#         list_infinitives.append('N/A')\n",
    "\n",
    "# print(list_infinitives)\n",
    "\n",
    "# container_results = driver.find_element(By.XPATH, '//*[@id=\"results\"]/div/ul')\n",
    "# #list_results = driver.find_elements(By.XPATH, '//*[@id=\"results\"]/div/ul/li')\n",
    "# first_result = driver.find_element(By.XPATH, '//*[@id=\"results\"]/div/ul/li[1]')\n",
    "# list_infinitives = []\n",
    "# #for result in list_results:\n",
    "# first_result.click()\n",
    "# pop_up_window = driver.switch_to.window(driver.window_handles[1])\n",
    "#     #frame_xpath = driver.find_element(By.XPATH, '/html/body/iframe')\n",
    "#     #driver.switch_to.frame(frame_xpath)\n",
    "# try:\n",
    "#     accept_cookies = driver.find_element(By.XPATH, '/html/body/div[5]/div/div[1]/div/div/div[2]/a[1]')\n",
    "#     accept_cookies.click()\n",
    "# except NoSuchElementException:\n",
    "#     pass\n",
    "# time.sleep(2)\n",
    "# infinitive = driver.find_element(By.XPATH, '//h1')\n",
    "# list_infinitives.append(infinitive.text)\n",
    "# driver.close()\n",
    "# driver.switch_to.window(driver.window_handles[0])\n",
    "# print(list_infinitives)\n",
    "\n",
    "#get_frequency(url)\n",
    "\n",
    "# button = driver.find_element(By.XPATH, '//button[@id=\"__BVID__72__BV_toggle_\"]')\n",
    "# button.click()\n",
    "# #dropdown = driver.find_element(By.XPATH, '//button[@id=\"__BVID__72\"]')\n",
    "# #li = driver.find_element(By.XPATH, '//li[@role=\"presentation\"][6]')\n",
    "# #WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, '//li[@role=\"presentation\"][6]'))).click()\n",
    "# #button_dropdown = li.find_element(By.XPATH, './button').click()\n",
    "# select = Select(driver.find_element(By.XPATH, '//li[@role=\"presentation\"][6]'))\n",
    "# button_dropdown = select.select_by_visible_text('Frequencia')\n",
    "# button_dropdown.click()\n",
    "\n",
    "# select by visible text\n",
    "# select.select_by_visible_text('Banana')\n",
    "# WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, '/button[@role=\"menu-item\"]'))).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "def fibonacci(n):\n",
    "    # Simple base case. In this case if n is 0, it return 0, and if it is 1, it return 1.\n",
    "    # Remember that, when you call for fibonacci(2), you will call for fibonacci(1) and fibonacci(0)\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    # Recurrence relation\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "print(fibonacci(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def get_info_from_html(self):\n",
    "    #     self.name_list = []\n",
    "    #     self.date_list = []\n",
    "    #     self.creator_list =[]\n",
    "        \n",
    "    #     for link in self.link_list:\n",
    "    #         self.get_html(link)\n",
    "    #         name = self.soup.find('h1').text\n",
    "    #         self.name_list.append(name)\n",
    "\n",
    "    #         date = self.soup.find('span', {\"class\":\"published-date\"}).text\n",
    "    #         self.date_list.append(date)\n",
    "\n",
    "    #         creator_name = self.soup.find('a', {'data-axl':\"alias\"}).text\n",
    "    #         self.creator_list.append(creator_name)\n",
    "\n",
    "    # def get_info_from_java(self):\n",
    "    #     self.num_supporters_list = []\n",
    "    #     self.num_days_remaining_list = []\n",
    "    #     for link in self.link_list:\n",
    "    #         self.driver.get(link)\n",
    "    #         soup = bs(self.driver.page_source, 'html.parser')\n",
    "    #         numbers = soup.findAll('div', class_= \"count\")\n",
    "    #         self.num_supporters_list.append(numbers[0].text)\n",
    "    #         self.num_days_remaining_list.append(numbers[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def collate_data(self):\n",
    "    #     self.info = {\n",
    "    #     # \"id\": self.link_id,\n",
    "    #     #         \"uuid\": self.link_uuid,\n",
    "    #             \"adj_rank-word-frequency\": self.adj_frequency_list,\n",
    "    #             #\"adj_phrases\": self.adj_phrase_list,\n",
    "    #             \"verb_rank-word-frequency\": self.verb_frequency_list,\n",
    "    #             \"verb_phrases\": self.verb_phrase_list}\n",
    "    #     print(self.info)\n",
    "\n",
    "        # def get_info(self, XPATH_name, XPATH_date, XPATH_creator, XPATH_supporters, XPATH_days):\n",
    "    #     self.name_list = []\n",
    "    #     self.date_list = []\n",
    "    #     self.creator_list =[]\n",
    "    #     self.num_supporters_list = []\n",
    "    #     self.num_days_remaining_list = []\n",
    "    #     for link in self.link_list:\n",
    "    #         self.open_url(link)\n",
    "    #         self.get_details(lst_name = self.name_list, XPATH= XPATH_name)\n",
    "    #         self.get_details(lst_name = self.date_list, XPATH= XPATH_date)\n",
    "    #         self.get_details(lst_name = self.creator_list, XPATH= XPATH_creator)\n",
    "    #         self.get_details(lst_name = self.num_supporters_list, XPATH= XPATH_supporters)\n",
    "    #         self.get_details(lst_name = self.num_days_remaining_list, XPATH= XPATH_days)\n",
    "    #         # name = self.driver.find_element(By.XPATH, XPATH_name)\n",
    "    #         # self.name_list.append(name)\n",
    "    #         # date = self.driver.find_element(By.XPATH, XPATH_date)\n",
    "    #         # self.name_list.append(date)\n",
    "    \n",
    "    # def get_details(self, XPATH, lst_name):\n",
    "    #     detail = self.driver.find_element(By.XPATH, XPATH)\n",
    "    #     lst_name.append(detail)\n",
    "    \n",
    "        # def collate_info(self):\n",
    "    #     self.info = {\"id\": self.link_id,\n",
    "    #             \"uuid\": self.link_uuid,\n",
    "    #             \"URL\": self.link_list,\n",
    "    #             \"idea_name\": self.name_list,\n",
    "    #             \"date\": self.date_list,\n",
    "    #             \"creator\": self.creator_list,\n",
    "    #             \"number_of_supporters\": self.num_supporters_list,\n",
    "    #             \"number_of_days_remaining\": self.num_days_remaining_list,\n",
    "    #             \"image_links\": self.img_list}\n",
    "    #     print(self.info) #delete later\n",
    "    #     return self.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # '''Uses self and works'''\n",
    "    # def get_name_date_creator(self, link):\n",
    "    #     self.name = self.find_in_html(link, 'h1', None, None)\n",
    "    #     self.try_append(self.name_list, self.name)\n",
    "\n",
    "    #     date = self.find_in_html(link, 'span', 'class', 'published-date')\n",
    "    #     self.try_append(self.date_list, date)\n",
    "\n",
    "    #     creator_name = self.find_in_html(link, 'a', 'data-axl', 'alias')\n",
    "    #     self.stripped_creator_name = creator_name.strip()\n",
    "    #     self.try_append(self.creator_list, self.stripped_creator_name)\n",
    "    \n",
    "    # '''Uses self (and works)'''\n",
    "    # def create_id(self):\n",
    "    #     ID = f'{self.name}.{self.stripped_creator_name}'\n",
    "    #     self.id_list.append(ID)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
